编写爬虫代码步骤：
1.准备
    1.明确目标与需求
        1.确定爬取的地址（网站）
        2.所需的具体数据
    2.分析目标网页
        1.用开发者工具（F12）分析页面结构，判断数据是 静态HTML 还是动态加载（如AJAX接口返回的JSON）
        2.定位数据位置：找到数据对应的标签（div、span等）
2.开发
    1.发送网络请求
        模拟浏览器发送请求，携带必要的Headers（Cookie等）
    2.解析响应数据
        静态HTML：用解析库（beaulifulsoup、lxml等） 或 通过 XPath、CSS 选择器定位内容。
        动态接口（JSON/XML）：直接解析响应文本，提取键值对中的数据（如response.json()）
    3.处理与存储数据
        数据清洗
        存储方式（.txt、.csv、数据库(MySQL/Redis)、Excel等）
3.优化与维护
    1.应对反爬机制
        控制请求频率（添加time.sleep()）避免被封禁IP
        动态更换User-Agent、使用代理IP池
        处理验证码（手动输入或集成识别工具）、破解动态参数（如分析JS生成的sign、token）
    2.测试与调试
        测试 单页爬取、分页爬取，检查数据完整性，格式
        处理异常情况
    3.持续维护
        更新代码

爬虫步骤：
1.确定目标
    1.文本类
        HTML，TXT，Markdown
    2.结构化数据
        JSON，XML，CSV
    3.媒体文件格式
        图片：JPG、PNG、GIF 等
        音频 / 视频：MP3、MP4、FLV 等
    4.二进制文件
        文档：PDF、Word(.docx)、Excel(.xlsx) 等
        压缩包：ZIP、RAR 等
2.分析网页
    1.页面结构分析
        1.手动：Ctrl+U（查看页面源码）；检查元素（开发者工具 F12）
        2.第三方工具
            1.爬虫测试工具： Postman（模拟请求并查看响应）；curl（命令行发送请求）
            2.解析测试工具：XPath Helper（浏览器插件，快速验证 XPath 表达式）；BeautifulSoup 在线测试工具
    2.请求分析
        1.确定请求类型与目标 URL
        2.分析请求参数
        3.解析请求头（Headers）
        4. 分析响应内容与格式
        5. 识别反爬机制相关特征
    3.反爬机制分析
3.发送请求（url、headers、response等）
4.处理响应（状态码检查：200，编码格式，内容提取：HTML\JSON等）
5.解析数据（HTML解析，JSON解析）
6.数据存储
7.异常处理与优化
